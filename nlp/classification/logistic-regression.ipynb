{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/masgari/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/masgari/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"twitter_samples\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preporcess the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet: str):\n",
    "    stemer = PorterStemmer()\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    stopwords_english = stopwords.words(\"english\")\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords_english and \\\n",
    "            token not in string.punctuation and \\\n",
    "            not token.isdigit():\n",
    "            token = stemer.stem(token)\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count word frequncies in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freq(processed_tweets: np.ndarray, class_label: np.ndarray) -> Dict[Tuple[str, int], int]:\n",
    "    freq = dict()    \n",
    "    for processed_tweet, label in zip(processed_tweets, class_label):\n",
    "        for token in processed_tweet:            \n",
    "            count = freq.get((token, label), 0)\n",
    "            freq[(token, label)] = count + 1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets_raw = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets_raw = twitter_samples.strings('negative_tweets.json')\n",
    "positive_tweets_processed = list(map(lambda tweet: process_tweet(tweet), positive_tweets_raw))\n",
    "negative_tweets_processed = list(map(lambda tweet: process_tweet(tweet), negative_tweets_raw))\n",
    "all_tweets_processed = positive_tweets_processed + negative_tweets_processed\n",
    "Y = np.concatenate((np.ones(len(positive_tweets_raw), dtype=int), np.zeros(len(negative_tweets_raw), dtype=int)), axis=0)\n",
    "\n",
    "NUM_FEATURES = 3\n",
    "word_freq = build_freq(all_tweets_processed, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweets_processed: List[List[str]], word_freq: Dict):\n",
    "    m = len(tweets_processed)\n",
    "    features = np.zeros((m, NUM_FEATURES))\n",
    "\n",
    "    for i in range(m):\n",
    "        tweet = tweets_processed[i]\n",
    "\n",
    "        tweet_features = np.zeros(NUM_FEATURES)\n",
    "        tweet_features[0] = 1.0 # Bias\n",
    "\n",
    "        for token in tweet:\n",
    "            # Positive count\n",
    "            tweet_features[1] += word_freq.get((token, 1), 0)        \n",
    "            # Negative count\n",
    "            tweet_features[2] += word_freq.get((token, 0), 0)\n",
    "        features[i] = tweet_features\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_X = extract_features(all_tweets_processed, word_freq)\n",
    "dataset = np.concatenate((dataset_X, Y.reshape(-1,1)), axis=1)\n",
    "np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainX shape: (8000, 3)\n",
      "TrainY shape: (8000, 1)\n",
      "TestX shape: (2000, 3)\n",
      "TestY shape: (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "\n",
    "trainX = dataset[:train_size, :NUM_FEATURES]\n",
    "trainY = dataset[:train_size, NUM_FEATURES:NUM_FEATURES+1]\n",
    "testX = dataset[train_size:, :NUM_FEATURES]\n",
    "testY = dataset[train_size:, NUM_FEATURES:NUM_FEATURES+1]\n",
    "\n",
    "print(f\"TrainX shape: {trainX.shape}\")\n",
    "print(f\"TrainY shape: {trainY.shape}\")\n",
    "print(f\"TestX shape: {testX.shape}\")\n",
    "print(f\"TestY shape: {testY.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, theta, learning_rate, num_iters):\n",
    "    m = x.shape[0]\n",
    "    for i in range(num_iters):\n",
    "        z = np.dot(x, theta)\n",
    "        y_hat = sigmoid(z)\n",
    "\n",
    "        # cost function\n",
    "        J = np.sum(y * np.log(y_hat) + (y-1) * np.log(1-y_hat))\n",
    "        J *= -1/m\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Cost: {J}\")\n",
    "        \n",
    "        gholi = y_hat - y\n",
    "        grads = np.dot(x.T, gholi)\n",
    "        theta = theta - (grads * learning_rate) / m\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: -0.00571846423961955\n",
      "Cost: 0.03850607039953177\n",
      "Cost: 0.056567496206624775\n",
      "Cost: 0.06325654123129627\n",
      "Cost: 0.06493874144147337\n",
      "Cost: 0.06440079911211058\n",
      "Cost: 0.06290600719513659\n",
      "Cost: 0.06104297617263544\n",
      "Cost: 0.05908945327930741\n",
      "Cost: 0.057175114745481426\n",
      "Cost: 0.055357381744936804\n",
      "Cost: 0.05365797062221184\n",
      "Cost: 0.05208101281806302\n",
      "Cost: 0.050622223545285706\n",
      "Cost: 0.04927359541106453\n",
      "Cost: 0.0480258018009412\n",
      "Cost: 0.04686940752154669\n",
      "Cost: 0.04579545161136744\n",
      "Cost: 0.04479569893877493\n",
      "Cost: 0.04386271871335302\n",
      "Cost: 0.04298987503885662\n",
      "Cost: 0.04217127549181102\n",
      "Cost: 0.041401702436168114\n",
      "Cost: 0.040676540117089544\n",
      "Cost: 0.0399917041520908\n",
      "Cost: 0.039343576508503605\n",
      "Cost: 0.03872894713991522\n",
      "Cost: 0.0381449624404506\n",
      "Cost: 0.037589080166667115\n",
      "Cost: 0.0370590302478452\n",
      "Cost: 0.03655278082922442\n",
      "Cost: 0.03606850889697692\n",
      "Cost: 0.03560457487759335\n",
      "Cost: 0.035159500665099486\n",
      "Cost: 0.034731950594708315\n",
      "Cost: 0.034320714944661275\n",
      "Cost: 0.03392469560603129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.30493439e-07],\n",
       "       [ 8.27405344e-04],\n",
       "       [-7.57676736e-04]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set theta to random values\n",
    "theta = np.zeros((NUM_FEATURES, 1))\n",
    "theta = train(trainX, trainY, theta, 1e-9, 3700)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(testX, testY, theta):\n",
    "    y_hat = sigmoid(np.dot(testX, theta))\n",
    "    y_hat = (y_hat > 0.5).astype(int)\n",
    "    return np.sum((y_hat == testY).astype(int)) / testY.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9925"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(testX, testY, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tweet: str, theta, word_freq):\n",
    "    tweet_processed = process_tweet(tweet)\n",
    "    x = extract_features([tweet_processed], word_freq)\n",
    "    return np.squeeze(sigmoid(np.dot(x, theta)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9463323595959156\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "emoji = \" :)\"\n",
    "tweet = \"I'm exciting to announce that I'm joining Tesla\"\n",
    "y_hat = predict(tweet + emoji, theta, word_freq)\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print(\"Positive\")\n",
    "else:\n",
    "    print(\"Negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Although model does not predict correctly if there is no emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4544629936023083\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "y_hat = predict(tweet, theta, word_freq)\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print(\"Positive\")\n",
    "else:\n",
    "    print(\"Negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
